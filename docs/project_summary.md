# nano-RL é¡¹ç›®æ€»ç»“

## æ•´ä½“æ•°æ®æµ

```mermaid
flowchart LR
    subgraph Input
        A[Dataset]
    end
    
    subgraph DataProcessor
        B[List Sample]
        C[PromptBatch]
    end
    
    subgraph RolloutWorker
        D[vLLM Generate]
        E[RolloutBatch]
    end
    
    subgraph RewardComputer
        F[RewardBatch]
    end
    
    subgraph AdvantageComputer
        G[AdvantageBatch]
    end
    
    subgraph DataProcessor2[DataProcessor]
        H[TrainingBatch]
    end
    
    subgraph ModelWorker
        I[train_step]
        J[StepMetrics]
    end
    
    A --> B
    B -->|to_prompt_batch| C
    C --> D
    D --> E
    E -->|compute| F
    F -->|compute| G
    G -->|to_training_batch| H
    H --> I
    I --> J
    I -.->|get_weights| D
```

---

## æ•°æ®ç±»å‹æµè½¬

| é˜¶æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| æ•°æ®é›†è¾“å‡º | `List[Sample]` | prompt + ground_truth |
| vLLM è¾“å…¥ | `PromptBatch` | tokenized promptsï¼ˆæ—  paddingï¼‰ |
| vLLM è¾“å‡º | `RolloutBatch` | åˆ†ç»„çš„ rollout ç»“æœ |
| Reward è®¡ç®— | `RewardBatch` | rollout + rewards |
| Advantage è®¡ç®— | `AdvantageBatch` | å±•å¼€çš„æ ·æœ¬ + normalized advantage |
| è®­ç»ƒè¾“å…¥ | `TrainingBatch` | padded tensors |
| è®­ç»ƒè¾“å‡º | `StepMetrics` | loss, lr, gradient_norm |

---

## å·²å®Œæˆ âœ…

### Core æ¨¡å— (`nano_rl/core/`)

| æ–‡ä»¶ | å†…å®¹ | çŠ¶æ€ |
|------|------|------|
| [config.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/core/config.py) | `ModelConfig`, `GenerationConfig`, `TrainConfig`, `RLConfig` | âœ… |
| [types.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/core/types.py) | æ‰€æœ‰æ•°æ®ç±»å‹å®šä¹‰ | âœ… |
| [interfaces.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/core/interfaces.py) | æ‰€æœ‰ Protocol å®šä¹‰ | âœ… |
| [processor.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/core/processor.py) | `DataProcessor` å®ç° | âœ… |

### Workers æ¨¡å— (`nano_rl/workers/`)

| æ–‡ä»¶ | å†…å®¹ | çŠ¶æ€ |
|------|------|------|
| [rollout_worker.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/workers/rollout_worker.py) | vLLM æ¨ç† + æƒé‡æ›´æ–° | âœ… |
| [model_worker.py](file:///mnt/data/ganzehua/nano-RL/nano_rl/workers/model_worker.py) | æ¨¡å‹åŠ è½½ã€LoRAã€ä¼˜åŒ–å™¨ã€scheduler | âš ï¸ `train_step` æœªå®ç° |

---

## å¾…å®Œæˆ ğŸ“‹

### 1. ModelWorker.train_step å®ç°
- [ ] GRPO loss è®¡ç®—ï¼ˆæ”¯æŒå¤šç§ loss_typeï¼‰
- [ ] æ¢¯åº¦ç´¯ç§¯
- [ ] è¿”å› `StepMetrics`

### 2. RewardComputer å®ç°
- [ ] åŸºäº `RewardFunctionProtocol` çš„ wrapper
- [ ] æ”¯æŒè§„åˆ™ rewardï¼ˆformat checkã€answer matchingï¼‰

### 3. AdvantageComputer å®ç°  
- [ ] Group-level normalizationï¼ˆGRPO æ ¸å¿ƒï¼‰
- [ ] ä» `RewardBatch` æ„å»º `AdvantageBatch`

### 4. Trainer ä¸»å¾ªç¯
- [ ] ç¼–æ’ DataProcessor â†’ RolloutWorker â†’ RewardComputer â†’ AdvantageComputer â†’ ModelWorker
- [ ] æƒé‡åŒæ­¥é€»è¾‘
- [ ] Checkpoint ä¿å­˜
- [ ] WandB æ—¥å¿—

### 5. Dataset / DataLoader
- [ ] æ•°æ®é›†æŠ½è±¡
- [ ] Prompt æ ¼å¼åŒ–ï¼ˆchat template å¤–éƒ¨å¤„ç†ï¼‰

---

## é¡¹ç›®ç»“æ„

```
nano-RL/
â”œâ”€â”€ nano_rl/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # é…ç½®ç±»
â”‚   â”‚   â”œâ”€â”€ types.py         # æ•°æ®ç±»å‹
â”‚   â”‚   â”œâ”€â”€ interfaces.py    # Protocol å®šä¹‰
â”‚   â”‚   â””â”€â”€ processor.py     # DataProcessor å®ç°
â”‚   â””â”€â”€ workers/
â”‚       â”œâ”€â”€ rollout_worker.py   # vLLM Worker
â”‚       â””â”€â”€ model_worker.py     # PyTorch Worker
â”œâ”€â”€ tests/
â””â”€â”€ pyproject.toml
```

# ä¸ªäººæ€è·¯
datasetçš„è¾“å…¥æ˜¯jsonlçš„data_pathï¼Œè¾“å‡ºæ˜¯List[Sample]ï¼Œæ¯ä¸ªsampleåŒ…å«promptå’Œground_truth
æ¯ä¸ªbatch sizeï¼Œæ˜¯ä»dataloaderè¿”å›List[Sample],æ‰€ä»¥ä»–çš„collate fnæ˜¯æ— è½¬åŒ–çš„è¿”å›
